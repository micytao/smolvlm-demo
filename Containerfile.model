# Model Server Container - Uses llama.cpp server with CUDA support
FROM ghcr.io/ggerganov/llama.cpp:server-cuda

# Set working directory
WORKDIR /app

# Install curl and wget for health checks and model download
USER root
RUN apt-get update && apt-get install -y curl wget && apt-get clean && rm -rf /var/lib/apt/lists/*

# Create models and cache directories with proper permissions
RUN mkdir -p /mnt/models /tmp/cache/llama.cpp /tmp/cache/huggingface && \
    chown -R 1001:1001 /mnt/models /tmp/cache && \
    chmod -R 777 /tmp/cache

# Switch back to non-root user
USER 1001

# Set environment variables for cache directory
ENV LLAMA_CACHE_DIR=/tmp/cache/llama.cpp
ENV HF_HOME=/tmp/cache/huggingface
ENV HF_HUB_CACHE=/tmp/cache/huggingface
ENV XDG_CACHE_HOME=/tmp/cache
ENV TRANSFORMERS_CACHE=/tmp/cache/huggingface

# No need to download model - llama-server will fetch it from HuggingFace directly

# Use llama-server with HuggingFace integration
EXPOSE 8080

# Health check - llama.cpp server provides /health endpoint
HEALTHCHECK --interval=30s --timeout=10s --start-period=120s --retries=3 \
  CMD curl -f http://localhost:8080/health || exit 1

# Use llama-server with HuggingFace model loading
# The base image already has ENTRYPOINT ["/app/llama-server"], so we just need CMD with arguments
CMD ["--hf-repo", "ggml-org/SmolVLM-500M-Instruct-GGUF", "--port", "8080", "--host", "0.0.0.0"]
